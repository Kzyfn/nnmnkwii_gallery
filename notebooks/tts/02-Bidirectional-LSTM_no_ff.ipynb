{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join\n",
    "# 以下を、ダウンロードしたデモのディレクリを指すように、修正してください。\n",
    "HTS_DEMO_ROOT = join(expanduser(\"~\"), \"\", \"HTS-demo_NIT-ATR503-M001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTS_DEMO_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./scripts/copy_from_htsdemo.sh $HTS_DEMO_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "#sys.path.remove('/usr/local/lib/python3.7/site-packages')\n",
    "sys.path.append('/usr/local/.pyenv/versions/3.6.0/lib/python3.6/site-packages')\n",
    "sys.path.append('/Users/kazuya_yufune/.pyenv/versions/3.6.0/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "\n",
    "from nnmnkwii.datasets import FileDataSource, FileSourceDataset\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset, MemoryCacheDataset#これはなに？\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames\n",
    "from nnmnkwii.preprocessing import minmax, meanvar, minmax_scale, scale\n",
    "from nnmnkwii import paramgen\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "\n",
    "from os.path import join, expanduser, basename, splitext, basename, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyworld\n",
    "import pysptk\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this demo we construct datasets from pre-computed linguistic/duration/acoustic features because computing features from wav/label files on-demand are peformance heavy, particulary for acoustic features. See the following python script if you are interested in how we extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data/basic5000\"#NIT-ATR503/\"#\n",
    "test_size = 0.01 # This means 480 utterances for training data\n",
    "random_state = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ./scripts/prepare_features2.py $DATA_ROOT --use_phone_alignment --question_path=\"./data/questions_jp2.hed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data specification\n",
    "\n",
    "Almost same as Merlin's slt_arctic demo. The only difference is that frequency warping paramter `alpha` is set to 0.41, instead of 0.58. As far as I know 0.41 is the best parameter approximating mel-frequency axis for 16kHz-sampled audio signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgc_dim = 180#メルケプストラム次数　？？\n",
    "lf0_dim = 3#対数fo　？？ なんで次元が３？\n",
    "vuv_dim = 1#無声or 有声フラグ　？？\n",
    "bap_dim = 15#発話ごと非周期成分　？？\n",
    "\n",
    "duration_linguistic_dim = 438#question_jp.hed で、ラベルに対する言語特徴量をルールベースで記述してる\n",
    "acoustic_linguisic_dim = 442#上のやつ+frame_features とは？？\n",
    "duration_dim = 1\n",
    "acoustic_dim = mgc_dim + lf0_dim + vuv_dim + bap_dim #aoustice modelで求めたいもの\n",
    "\n",
    "fs = 48000\n",
    "frame_period = 5\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "hop_length = int(0.001 * frame_period * fs)\n",
    "\n",
    "mgc_start_idx = 0\n",
    "lf0_start_idx = 180\n",
    "vuv_start_idx = 183\n",
    "bap_start_idx = 184\n",
    "\n",
    "windows = [\n",
    "    (0, 0, np.array([1.0])),\n",
    "    (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "    (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "]\n",
    "\n",
    "use_phone_alignment = True\n",
    "acoustic_subphone_features = \"coarse_coding\" if use_phone_alignment else \"full\" #とは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File data sources\n",
    "\n",
    "We need to specify 1) where to find pre-computed features and 2) how to process them. In this case,\n",
    "\n",
    "1. `collect_files` : Collects `.bin` files. External python script writes files in binary format. Also we split the files into train/test set.\n",
    "2. `collect_features` : Just load from file by `np.fromfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFileSource(FileDataSource):\n",
    "    def __init__(self, data_root, dim, train):\n",
    "        self.data_root = data_root\n",
    "        self.dim = dim\n",
    "        self.train = train\n",
    "    def collect_files(self):\n",
    "        files = sorted(glob(join(self.data_root, \"*.bin\")))\n",
    "        #files = files[:4000]#[:len(files)-5] # last 5 is real testset\n",
    "        train_files = files#[:-5]\n",
    "        test_files = files[-5:]\n",
    "        #train_files, test_files = train_test_split(files, test_size=test_size,\n",
    "                                                  # random_state=random_state)\n",
    "        if self.train:\n",
    "            return train_files\n",
    "        else:\n",
    "            return test_files\n",
    "    def collect_features(self, path):\n",
    "        return np.fromfile(path, dtype=np.float32).reshape(-1, self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {\"duration\":{}, \"acoustic\": {}}\n",
    "Y = {\"duration\":{}, \"acoustic\": {}}\n",
    "utt_lengths = {\"duration\":{}, \"acoustic\": {}}\n",
    "for ty in [ \"acoustic\"]:\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        train = phase == \"train\"\n",
    "        x_dim = duration_linguistic_dim if ty == \"duration\" else acoustic_linguisic_dim\n",
    "        y_dim = duration_dim if ty == \"duration\" else acoustic_dim\n",
    "        X[ty][phase] = FileSourceDataset(BinaryFileSource(join(DATA_ROOT, \"X_{}\".format(ty)),\n",
    "                                                       dim=x_dim,\n",
    "                                                       train=train))\n",
    "        Y[ty][phase] = FileSourceDataset(BinaryFileSource(join(DATA_ROOT, \"Y_{}\".format(ty)),\n",
    "                                                       dim=y_dim,\n",
    "                                                       train=train))\n",
    "        utt_lengths[ty][phase] = np.array([len(x) for x in X[ty][phase]], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['acoustic']['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['acoustic']['train'][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct datasets for duration and acoustic models. We wil have\n",
    "\n",
    "- X: Input (duration, acoustic) datasets\n",
    "- Y: Target (duration, acoustic) datasets\n",
    "\n",
    "Note that dataset itself doesn't keep features in memory. It loads features on-demand while iteration or indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mini-batch sequenceial training (which should be drastically faster than batch_size=1), later we will use [PackedSequence](http://pytorch.org/docs/master/nn.html#torch.nn.utils.rnn.PackedSequence) in PyTorch. For this, re-create datasets with `PaddedFileSourceDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for ty in [ \"acoustic\"]:\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        train = phase == \"train\"\n",
    "        x_dim = duration_linguistic_dim if ty == \"duration\" else acoustic_linguisic_dim\n",
    "        y_dim = duration_dim if ty == \"duration\" else acoustic_dim\n",
    "        X[ty][phase] = FileSourceDataset(BinaryFileSource(join(DATA_ROOT, \"X_{}\".format(ty)),\n",
    "                                                       dim=x_dim,\n",
    "                                                       train=train))\n",
    "        Y[ty][phase] = FileSourceDataset(BinaryFileSource(join(DATA_ROOT, \"Y_{}\".format(ty)),\n",
    "                                                       dim=y_dim,\n",
    "                                                       train=train), )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['acoustic']['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterance lengths\n",
    "\n",
    "Let's see utterance lengths histrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of utterances:\", len(utt_lengths[\"acoustic\"][\"train\"]))\n",
    "print(\"Total number of frames:\", np.sum(utt_lengths[\"acoustic\"][\"train\"]))\n",
    "hist(utt_lengths[\"acoustic\"][\"train\"], bins=64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How data look like?\n",
    "\n",
    "Pick an utterance from training data and visualize its features.\n",
    "\n",
    "## ここ本気でわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_min = {}\n",
    "X_max = {}\n",
    "Y_mean = {}\n",
    "Y_var = {}\n",
    "Y_scale = {}\n",
    "\n",
    "for typ in [\"acoustic\"]:\n",
    "    X_min[typ], X_max[typ] = minmax(X[typ][\"train\"], utt_lengths[typ][\"train\"])\n",
    "    Y_mean[typ], Y_var[typ] = meanvar(Y[typ][\"train\"], utt_lengths[typ][\"train\"])\n",
    "    Y_scale[typ] = np.sqrt(Y_var[typ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguistic features should be clear with normalization as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tnrange, tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join(DATA_ROOT+'/mora_index', \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mora_index_lists = ['data/basic5000/mora_index/mora_index_' + '0'*(4-len(str(i+1)))  + str(i+1) + '.csv' for i in range(5000)]\n",
    "mora_index_lists = mora_index_lists#[:len(mora_index_lists)-5] # last 5 is real testset\n",
    "\n",
    "mora_index_lists_for_model = [np.loadtxt(path).reshape(-1) for path in mora_index_lists]\n",
    "\n",
    "\n",
    "\n",
    "#train_mora_index_lists, test_mora_index_lists = train_test_split(mora_index_lists_for_model, test_size=test_size,\n",
    "#                                                  random_state=random_state)\n",
    "\n",
    "train_mora_index_lists = mora_index_lists_for_model[:5000]\n",
    "#test_mora_index_lists = mora_index_lists_for_model[4000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index_ = train_test_split(list(range(4995)), test_size=0.01, random_state =1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_validation_indices = np.random.choice(test_index_, 4, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_parameters(y_predicted):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    plt.plot(lf0[:, 0])\n",
    "    plt.show()\n",
    "    #lf0 = Y['acoustic']['train'][90][:, lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.zeros(lf0.shape)\n",
    "    vuv = y_predicted[:,vuv_start_idx]\n",
    "\n",
    "    plt.show()\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(Y_var[ty][:lf0_start_idx], (T, 1))#np.tile(Y_var[ty][:lf0_start_idx], (T, 1))\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(Y_var[ty][lf0_start_idx:vuv_start_idx], (T,1))\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(Y_var[ty][bap_start_idx:], (T, 1))\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap\n",
    "def gen_waveform(y_predicted, do_postfilter=False):  \n",
    "    y_predicted = trim_zeros_frames(y_predicted)\n",
    "        \n",
    "    # Generate parameters and split streams\n",
    "    mgc, lf0, vuv, bap = gen_parameters(y_predicted)\n",
    "    \n",
    "    if do_postfilter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "        \n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), fs, fftlen)\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "    \n",
    "    generated_waveform = pyworld.synthesize(f0.flatten().astype(np.float64),\n",
    "                                            spectrogram.astype(np.float64),\n",
    "                                            aperiodicity.astype(np.float64),\n",
    "                                            fs, frame_period)\n",
    "    return generated_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_layers=1, z_dim=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_direction =  2 if bidirectional else 1\n",
    "        self.z_dim = z_dim\n",
    "        self.lstm1 = nn.LSTM(acoustic_linguisic_dim+acoustic_dim, 400, 1 if num_layers==2 and (z_dim == 32 or z_dim==2) else num_layers , bidirectional=bidirectional, dropout=dropout)#入力サイズはここできまる\n",
    "        self.fc21 = nn.Linear(self.num_direction*400, z_dim)\n",
    "        self.fc22 = nn.Linear(self.num_direction*400, z_dim)\n",
    "        ##ここまでエンコーダ\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(acoustic_linguisic_dim+z_dim, 400, num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc3 = nn.Linear(self.num_direction*400, acoustic_dim)\n",
    "\n",
    "    def encode(self, linguistic_f, acoustic_f, mora_index):\n",
    "        x = torch.cat([linguistic_f, acoustic_f], dim=1)\n",
    "        out, hc = self.lstm1(x.view( x.size()[0],1, -1))\n",
    "        nonzero_indices = torch.nonzero(mora_index.view(-1).data).squeeze()\n",
    "        out = out[nonzero_indices]\n",
    "        del nonzero_indices\n",
    "        \n",
    "        h1 = F.relu(out)\n",
    "\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, linguistic_features, mora_index, z_dsiabled=False, z0 = True):\n",
    "        \n",
    "        z_tmp = torch.tensor([[0]*self.z_dim]*linguistic_features.size()[0], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        count = 0\n",
    "        prev_index = 0\n",
    "        for i, mora_i in enumerate(mora_index):\n",
    "            if mora_i == 1:\n",
    "                z_tmp[prev_index:i] = z[count]\n",
    "                prev_index = i\n",
    "                count += 1\n",
    "                \n",
    "        if z_dsiabled:\n",
    "            tmp = 0 if z0 else 1\n",
    "            z_tmp = torch.tensor([[tmp]*self.z_dim]*linguistic_features.size()[0], dtype=torch.float32).to('cpu')\n",
    "                \n",
    "\n",
    "        \n",
    "        x = torch.cat([linguistic_features, z_tmp.view(-1, self.z_dim)], dim=1).view(linguistic_features.size()[0], 1, -1)\n",
    "        h3, (h, c) = self.lstm2(x)\n",
    "        h3 = F.relu(h3)\n",
    "        \n",
    "        return self.fc3(h3)#torch.sigmoid(self.fc3(h3))\n",
    "\n",
    "    def forward(self, linguistic_features, acoustic_features, mora_index, z_dsiabled = False, z0=True):\n",
    "        mu, logvar = self.encode(linguistic_features, acoustic_features, mora_index)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.decode(z, linguistic_features, mora_index, z_dsiabled = z_dsiabled, z0 = z0), mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 798#0#\n",
    "#プロットするときはアクセントラベルの方がいいかも\n",
    "#f0使いたいならタイムフレーム軸で、zは線形補完\n",
    "\n",
    "#x_train = minmax_scale(x_train, x_min, x_max, feature_range=(0.01, 0.99))\n",
    "#y_train = scale(y_train, y_mean, y_scale)\n",
    "\n",
    "\n",
    "def check_reconstruction(test_index, z0=False):\n",
    "    with torch.no_grad():\n",
    "        x_train = X['acoustic']['train'][test_index]\n",
    "        y_train = Y['acoustic']['train'][test_index]\n",
    "        mora_i = train_mora_index_lists[test_index]\n",
    "        x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "        mora_i = torch.from_numpy(mora_i).type(torch.FloatTensor)\n",
    "        mora_nonzero_index = torch.nonzero(mora_i.view(-1).data).squeeze()\n",
    "\n",
    "\n",
    "        reconstruction, mu, logvar = model(x_train, y_train, mora_i)\n",
    "\n",
    "        nonzero_indices = torch.nonzero(mora_i.view(-1).data).squeeze()\n",
    "        prev_i = 0\n",
    "\n",
    "        #言語特徴量、音響特徴量、mora_index\n",
    "        true_accent = Y['acoustic']['train'][test_index][:, lf0_start_idx]#[np.where(train_mora_index_lists[test_index]>0)][:, lf0_start_idx]\n",
    "        pred_accent = reconstruction.view(-1, 199).detach().numpy()[:, lf0_start_idx]#[np.where(train_mora_index_lists[test_index]>0)][:, lf0_start_idx]\n",
    "        pred_accent = (pred_accent - true_accent.mean()) / true_accent.std()\n",
    "        true_accent = (true_accent -true_accent.mean()) / true_accent.std()\n",
    "\n",
    "        true_accent_hl = np.loadtxt('data/basic5000/accents/accents_' + '0'*(4-len(str(test_index)))  + str(test_index) + '.csv')\n",
    "\n",
    "        \"\"\"\n",
    "        pred = mu.detach().numpy().reshape(-1)\n",
    "        plt.plot(true_accent_hl, label='true accent')\n",
    "        plt.plot(pred, label='z')\n",
    "        plt.xlabel('mora_index')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        #print('相関係数')\n",
    "        #print(np.corrcoef([pred,true_accent_hl ])[0][1])\n",
    "\n",
    "\n",
    "        pred_for_plot = torch.tensor([0]*pred_accent.shape[0], dtype=torch.float32)\n",
    "        prev_index = 0\n",
    "\n",
    "        for i, z in enumerate(mora_nonzero_index):\n",
    "            pred_for_plot[prev_index:int(z)] = torch.tensor(pred[i])\n",
    "            prev_index = int(z)\n",
    "\n",
    "        pred_for_plot = pred_for_plot.numpy()\n",
    "        plt.plot(true_accent, label='real lf0')\n",
    "        plt.plot(pred_accent, label='pred lf0')\n",
    "        #plt.plot(pred_for_plot, label='z')\n",
    "        #plt.errorbar(range(logvar.size()[0]), mu.detach().numpy().reshape(-1), yerr = logvar.detach().numpy().reshape(-1), label='z')\n",
    "        plt.legend()\n",
    "        plt.xlabel('time frame')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #zが普通\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"z normal\")\n",
    "        IPython.display.display(Audio(gen_waveform(reconstruction.view(-1, 199).detach().numpy(), True), rate=fs))\n",
    "        if z0:\n",
    "            reconstruction_, mu_, logvar_ = model(x_train, y_train, mora_i, z_dsiabled=True)\n",
    "            print(\"z0\")\n",
    "            IPython.display.display(Audio(gen_waveform(reconstruction_.view(-1, 199).detach().numpy(), True), rate=fs))\n",
    "            print(\"z1\")\n",
    "            reconstruction_, mu_, logvar_ = model(x_train, y_train, mora_i, z_dsiabled=True, z0=False)\n",
    "            IPython.display.display(Audio(gen_waveform(reconstruction_.view(-1, 199).detach().numpy(), True), rate=fs))\n",
    "\n",
    "            IPython.display.display(Audio(gen_waveform(Y['acoustic']['train'][test_index], True), rate=fs))\n",
    "\n",
    "\n",
    "    return mu, logvar, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "z_dim = 1\n",
    "dropout=0.3 \n",
    "model = VAE(num_layers=num_layers, z_dim=z_dim).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#junk = 'junk_models/vae_mse_0.01kld_z_changed_losssum_batchfirst_20.pth'\n",
    "paths = sorted(glob('optuna0612/{}layers_zdim{}_model*'.format(num_layers, z_dim)))\n",
    "epochs = []\n",
    "for path in paths:\n",
    "    epoch = int(path[path.index('l_')+2:path.index('.')])\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "epoch_index = np.array(epochs).argmax()\n",
    "\n",
    "#model.load_state_dict(torch.load('optuna0612/{}layers/zdim{}/vae.pth'.format(num_layers, z_dim), map_location=torch.device('cpu') ))\n",
    "model.load_state_dict(torch.load(paths[epoch_index], map_location=torch.device('cpu') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(A, B) :\n",
    "    return (np.square(A - B)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [i*20+1 for i in range(250)]\n",
    "import random\n",
    "random.seed(12345)#12345\n",
    "randomli_sampled_test = random.sample(test_indices, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se = 0\n",
    "f0_e = 0\n",
    "for test_index in tqdm(test_indices):\n",
    "    x_train = X['acoustic']['train'][test_index]\n",
    "    y_train = Y['acoustic']['train'][test_index]\n",
    "    mora_i = train_mora_index_lists[test_index]\n",
    "    x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "    mora_i = torch.from_numpy(mora_i).type(torch.FloatTensor)\n",
    "    \n",
    "    \n",
    "    reconstruction, mu, logvar  = model(x_train, y_train, mora_i)\n",
    "    \n",
    "    del x_train\n",
    "    del y_train\n",
    "    del mora_i\n",
    "    \n",
    "    se += mse(Y['acoustic']['train'][test_index], reconstruction.detach().numpy())\n",
    "    f0_e += mse(Y['acoustic']['train'][test_index][:, lf0_start_idx:vuv_start_idx], reconstruction.detach().numpy().reshape(-1, 199)[:, lf0_start_idx:vuv_start_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(se)\n",
    "print(f0_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomli_sampled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2layers_zdim8\n",
    "mus = []\n",
    "logvars = []\n",
    "num_layers = 3\n",
    "z_dim = 8\n",
    "dropout=0.3 \n",
    "model = VAE(num_layers=num_layers, z_dim=z_dim).to('cpu')\n",
    "model.load_state_dict(torch.load('accent_models/{}layers/zdim{}/vae.pth'.format(num_layers, z_dim), map_location=torch.device('cpu') ))\n",
    "\n",
    "for index in randomli_sampled_test:\n",
    "    mu, logvar = check_reconstruction(index, z0=False)\n",
    "    mus.append(mu.detach().numpy())\n",
    "    logvars.append(logvar.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2layers_zdim32\n",
    "mus = []\n",
    "logvars = []\n",
    "num_layers = 2\n",
    "z_dim = 4\n",
    "dropout=0.3 \n",
    "model = VAE(num_layers=num_layers, z_dim=z_dim).to('cpu')\n",
    "model.load_state_dict(torch.load('accent_models/{}layers/zdim{}/vae.pth'.format(num_layers, z_dim), map_location=torch.device('cpu') ))\n",
    "\n",
    "for index in randomli_sampled_test:\n",
    "    mu, logvar = check_reconstruction(index, z0=False)\n",
    "    mus.append(mu.detach().numpy())\n",
    "    logvars.append(logvar.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zd_nl_combination = [[1,1], [1, 2], [1, 4], [1, 8], [1, 32], [2, 1], [2, 2], [2, 4], [2, 8], [2, 32], [3, 8]]\n",
    "f0_errors = []\n",
    "rmse_all = []\n",
    "\n",
    "for nl, zd in zd_nl_combination:\n",
    "    f0_error, mse_all = error_calculator(zd, nl, test_indices)\n",
    "    f0_errors.append(f0_error)\n",
    "    rmse_all.append(mse_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(rmse_all, zd_nl_combination):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(f0_errors, zd_nl_combination):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a - b) ** 2))\n",
    "\n",
    "def calc_lf0_rmse(natural, generated, lf0_idx, vuv_idx):\n",
    "    idx = (natural[:, vuv_idx] * (generated[:, vuv_idx] >= 0.5)).astype(bool)\n",
    "    return rmse(natural[idx, lf0_idx], generated[idx, lf0_idx]) * 1200 / np.log(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_calculator(z_dim, num_layers, test_indices):\n",
    "    model = VAE(num_layers=num_layers, z_dim=z_dim).to('cpu')\n",
    "    model.load_state_dict(torch.load('accent_models/{}layers/zdim{}/vae.pth'.format(num_layers, z_dim), map_location=torch.device('cpu')))\n",
    "    \n",
    "    f0_errors = []\n",
    "    mse_all = []\n",
    "    \n",
    "    for test_index in tqdm(test_indices):\n",
    "        with torch.no_grad():\n",
    "            x_train = X['acoustic']['train'][test_index]\n",
    "            y_train = Y['acoustic']['train'][test_index]\n",
    "            mora_i = train_mora_index_lists[test_index]\n",
    "            x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "            y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "            mora_i = torch.from_numpy(mora_i).type(torch.FloatTensor)\n",
    "            reconstruction, mu, logvar = model(x_train, y_train, mora_i)\n",
    "            y_train = y_train.numpy().reshape(-1, 199)\n",
    "            reconstruction = reconstruction.numpy().reshape(-1, 199)\n",
    "            \n",
    "        f0_errors.append(calc_lf0_rmse(y_train, reconstruction, lf0_start_idx, vuv_start_idx))\n",
    "        mse_all.append(rmse(y_train, reconstruction))\n",
    "        \n",
    "    return np.array(f0_errors).mean(), np.array(mse_all).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#0609は81と41??\n",
    "#良い 1, 81, 4641\n",
    "#悪い 41, 0, 4041\n",
    "test_index = 581\n",
    "mu, logvar = check_reconstruction(test_index, z0=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accent = np.loadtxt('data/basic5000/accents/accents_' + '0'*(4-len(str(test_index)))+ str(test_index) + '.csv')\n",
    "if z_dim == 1:\n",
    "    \n",
    "    pred_z = mu.detach().numpy().reshape(-1)\n",
    "    \n",
    "    xlabel = ['te', 'tsu', 'no', 'ka', 'a', 'te ', 'N', 'ga', 'yo', 'o', 'ro', 'cl', 'pa', 'ta', 'i', 'ri', 'ku ', 'ni', 'o ', 'ri ', 'ta ']\n",
    "    #xlabel = ['mi', 'zu', 'o', 'ma', 're', 'e', 'shi', 'a', 'ka', 'ra', 'ka ', 'wa', 'na ', 'ke', 're ', 'ba', 'na  ', 'ra ', 'na', 'i', 'no', 'de', 'su']\n",
    "    plt.title('basic' + '0'*(4-len(str(test_index+1)))+ str(test_index+1))\n",
    "    plt.plot(xlabel, test_accent, label='true_accent_label')\n",
    "    plt.errorbar(xlabel, pred_z, yerr= torch.exp(0.5*logvar).detach().numpy().reshape(-1), label='pred z')\n",
    "    plt.ylabel('accent_label')\n",
    "    plt.xlabel('mora index')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(test_accent, pred_z)\n",
    "    plt.xlabel('true accent label')\n",
    "    plt.ylabel('predicted z')\n",
    "    plt.show()\n",
    "\n",
    "    print('相関係数')\n",
    "    print(np.corrcoef(test_accent, pred_z)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_tmp = []\n",
    "labels = []\n",
    "test_accents = [np.loadtxt('data/basic5000/accents/accents_' + '0'*(4-len(str(test_index)))+ str(test_index) + '.csv') for test_index in randomli_sampled_test]\n",
    "mus = np.array(mus)\n",
    "for i in range(mus.shape[0]):\n",
    "    mu_tmp = mus[i].reshape(-1, z_dim)\n",
    "    for j in range(mu_tmp.shape[0]):\n",
    "        mus_tmp.append(mu_tmp[j])\n",
    "        labels.append(test_accents[i][j])\n",
    "\n",
    "mu_data = np.array(mus_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_data = np.array(mu_data)\n",
    "mu_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if z_dim ==1:\n",
    "    plt.scatter(labels, mu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主成分分析の実行\n",
    "pca = PCA()\n",
    "pca.fit(mu_data)\n",
    "# データを主成分空間に写像\n",
    "feature = pca.transform(mu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(feature[:, 0], feature[:, 1], c=labels, )\n",
    "plt.title(\"{}layers_zdim{}\".format(num_layers, z_dim))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = TSNE(n_components=2, random_state=0).fit_transform(mu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices = [random.randint(0, 5000) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1, 4 90, 798がいいzz感じ\n",
    "test_index = 798\n",
    "mu, logvar = check_reconstruction(test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index in random_validation_indices:\n",
    "    print(index)\n",
    "    mu, logvar = check_reconstruction(index)\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs = []\n",
    "for test_index in indices:\n",
    "    #test_index = 0\n",
    "    mu, logvar = check_reconstruction(test_index)\n",
    "    zs.append(mu.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = np.array(zs).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([[0, 0]])\n",
    "\n",
    "for i in range(10):\n",
    "    tmp_x = zs[i].reshape(-1, 2)\n",
    "    tmp = np.concatenate([tmp, tmp_x],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tmp[:, 0], tmp[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('mora_index')\n",
    "std = torch.exp(0.5*logvar[:, 1])\n",
    "plt.errorbar(range(logvar.size()[0]), mu, yerr = std.detach().numpy().reshape(-1), label='mu ± logvar')\n",
    "plt.ylabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_loader -> [X[acoustic][train],  Y[acoustic][train], mora_index] \n",
    "## ここの順番づけが終わればひと段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1), x.view(-1, ), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "#train_loader -> [X[acoustic][train],  Y[acoustic][train], mora_index] ここの順番づけが終わればひと段落\n",
    "\n",
    "func_tensor = np.vectorize(torch.from_numpy)\n",
    "\n",
    "X_acoustic_train = [torch.from_numpy(X['acoustic']['train'][i]) for i in range(len(X['acoustic']['train']))] \n",
    "Y_acoustic_train = [torch.from_numpy(Y['acoustic']['train'][i]) for i in range(len(Y['acoustic']['train']))]\n",
    "train_mora_index_lists = [torch.tensor(train_mora_index_lists[i]) for i in range(len(train_mora_index_lists))]\n",
    "\n",
    "X_acoustic_test = [torch.from_numpy(X['acoustic']['test'][i]) for i in range(len(X['acoustic']['test']))]\n",
    "Y_acoustic_test = [torch.from_numpy(Y['acoustic']['test'][i]) for i in range(len(Y['acoustic']['test']))]\n",
    "test_mora_index_lists = [torch.tensor(test_mora_index_lists[i]) for i in range(len(test_mora_index_lists))]\n",
    "\n",
    "train_loader = [[X_acoustic_train[i], Y_acoustic_train[i], train_mora_index_lists[i]] for i in range(len(train_mora_index_lists))]\n",
    "test_loader = [[X_acoustic_test[i], Y_acoustic_test[i], test_mora_index_lists[i]] for i in range(len(test_mora_index_lists))]\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        for j in range(3):\n",
    "            data[j] = data[j].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data[0], data[1], data[2])\n",
    "        loss = loss_function(recon_batch, data[1], mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / 1))\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data, in enumerate(test_loader):\n",
    "            for j in range(3):\n",
    "                data[j] = data[j].to(device)\n",
    "            recon_batch, mu, logvar = model(data[0], data[1], data[2])\n",
    "            test_loss += loss_function(recon_batch, data[1], mu, logvar).item()\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "            \"\"\"\n",
    "\n",
    "    test_loss /= 1\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(test_mora_index_lists):\n",
    "    if x.size()[0] >= 1900:\n",
    "        print(x.size())\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(X_acoustic_test):\n",
    "    if x.size()[0] >= 1900:\n",
    "        print(x.size())\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['acoustic']['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    print(loss)\n",
    "    print(test_loss)\n",
    "\n",
    "    print('epoch [{}/{}], loss: {:.4f} test_loss: {:.4f}'.format(\n",
    "        epoch + 1,\n",
    "        num_epochs,\n",
    "        loss,\n",
    "        test_loss))\n",
    "\n",
    "    # logging\n",
    "    loss_list.append(loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "# save the training model\n",
    "np.save('loss_list.npy', np.array(loss_list))\n",
    "np.save('test_loss_list.npy', np.array(test_loss_list))\n",
    "torch.save(model.state_dict(), 'vae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Configurations\n",
    "\n",
    "Network hyper parameters and training configurations (learning rate, weight decay, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 3\n",
    "hidden_size = 512\n",
    "\n",
    "batch_size = 8#発話\n",
    "\n",
    "n_workers = 2\n",
    "pin_memory = True\n",
    "nepoch = 10\n",
    "lr = 0.002\n",
    "weight_decay = 1e-6\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainining loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RNN predicts output feature sequence given a input feature sequence, so we need to feed our data to network in sequence-wise manner. This is pretty easy. We can just use `MemoryCacheDataset` that supports utterancew-wise iteration and has cache functionality to avoid file re-loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, optimizer, X, Y, X_min, X_max, Y_mean, Y_scale,\n",
    "          utt_lengths, cache_size=1000):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    X_train, X_test = X[\"train\"], X[\"test\"]\n",
    "    Y_train, Y_test = Y[\"train\"], Y[\"test\"]\n",
    "    train_lengths, test_lengths = utt_lengths[\"train\"], utt_lengths[\"test\"]\n",
    "    \n",
    "    # Sequence-wise train loader\n",
    "    X_train_cache_dataset = MemoryCacheDataset(X_train, cache_size)\n",
    "    Y_train_cache_dataset = MemoryCacheDataset(Y_train, cache_size)\n",
    "    train_dataset = PyTorchDataset(X_train_cache_dataset, Y_train_cache_dataset, train_lengths,\n",
    "                                  X_min, X_max, Y_mean, Y_scale)\n",
    "    train_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "    \n",
    "    # Sequence-wise test loader\n",
    "    X_test_cache_dataset = MemoryCacheDataset(X_test, cache_size)\n",
    "    Y_test_cache_dataset = MemoryCacheDataset(Y_test, cache_size)\n",
    "    test_dataset = PyTorchDataset(X_test_cache_dataset, Y_test_cache_dataset, test_lengths,\n",
    "                                 X_min, X_max, Y_mean, Y_scale)\n",
    "    test_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "    \n",
    "    dataset_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "        \n",
    "    # Training loop\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    print(\"Start utterance-wise training...\")\n",
    "    loss_history = {\"train\": [], \"test\": []}\n",
    "    for epoch in tnrange(nepoch):\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            running_loss = 0\n",
    "            for x, y, lengths in dataset_loaders[phase]:\n",
    "                # Sort by lengths . This is needed for pytorch's PackedSequence\n",
    "                sorted_lengths, indices = torch.sort(lengths.view(-1), dim=0, descending=True)\n",
    "                sorted_lengths = sorted_lengths.long().numpy()\n",
    "                # Get sorted batch\n",
    "                x, y = x[indices], y[indices]\n",
    "                # Trim outputs with max length\n",
    "                y = y[:, :sorted_lengths[0]]\n",
    "                \n",
    "                # Init states\n",
    "                h, c = model.init_hidden(len(sorted_lengths))\n",
    "                if use_cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                if use_cuda:\n",
    "                    h, c = h.cuda(), c.cuda()\n",
    "                x, y = Variable(x), Variable(y)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Do apply model for a whole sequence at once\n",
    "                # no need to keep states\n",
    "                y_hat = model(x, sorted_lengths, h, c)\n",
    "                loss = criterion(y_hat, y)\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            loss_history[phase].append(running_loss / (len(dataset_loaders[phase])))\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "length = {'duration': 531, 'acoustic': 535}\n",
    "for typ in [\"duration\", \"acoustic\"]:\n",
    "    models[typ] = MyRNN(length[typ],\n",
    "                            hidden_size, Y[typ][\"train\"][0].shape[-1],\n",
    "                            num_hidden_layers, bidirectional=True)\n",
    "    print(\"Model for {}\\n\".format(typ), models[typ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['duration'].load_state_dict(torch.load('models_basic5000/model_duration', map_location=torch.device('cpu')))\n",
    "models['acoustic'].load_state_dict(torch.load(\"models_basic5000/model_acoustic\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Duration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty = \"duration\"\n",
    "optimizer = optim.Adam(models[ty].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_history = train_rnn(models[ty], optimizer, X[ty], Y[ty],\n",
    "                     X_min[ty], X_max[ty], Y_mean[ty], Y_scale[ty], utt_lengths[ty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(loss_history[\"train\"], linewidth=2, label=\"Train loss\")\n",
    "plot(loss_history[\"test\"], linewidth=2, label=\"Test loss\")\n",
    "legend(prop={\"size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training acoustic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty = \"acoustic\"\n",
    "optimizer = optim.Adam(models[ty].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_history = train_rnn(models[ty], optimizer, X[ty], Y[ty],\n",
    "                     X_min[ty], X_max[ty], Y_mean[ty], Y_scale[ty], utt_lengths[ty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(loss_history[\"train\"], linewidth=2, label=\"Train loss\")\n",
    "plot(loss_history[\"test\"], linewidth=2, label=\"Test loss\")\n",
    "legend(prop={\"size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Let's see how our network works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter generation utilities\n",
    "\n",
    "Almost same as DNN text-to-speech synthesis. The difference is that we need to give initial hidden states explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dict, continuous_dict = hts.load_question_set(\"./data/questions_jp2.hed\")\n",
    "\n",
    "def gen_parameters(y_predicted):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.zeros(lf0.shape)\n",
    "    vuv = y_predicted[:,vuv_start_idx]\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(Y_var[ty][:lf0_start_idx], (T, 1))\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(Y_var[ty][lf0_start_idx:vuv_start_idx], (T,1))\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(Y_var[ty][bap_start_idx:], (T, 1))\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap\n",
    "\n",
    "def gen_waveform(y_predicted, do_postfilter=False):  \n",
    "    y_predicted = trim_zeros_frames(y_predicted)\n",
    "        \n",
    "    # Generate parameters and split streams\n",
    "    mgc, lf0, vuv, bap = gen_parameters(y_predicted)\n",
    "    \n",
    "    if do_postfilter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "        \n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), fs, fftlen)\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "    \n",
    "    generated_waveform = pyworld.synthesize(f0.flatten().astype(np.float64),\n",
    "                                            spectrogram.astype(np.float64),\n",
    "                                            aperiodicity.astype(np.float64),\n",
    "                                            fs, frame_period)\n",
    "    return generated_waveform\n",
    "    \n",
    "def gen_duration(hts_labels, duration_model):\n",
    "    # Linguistic features for duration\n",
    "    duration_linguistic_features = fe.linguistic_features(hts_labels,\n",
    "                                               binary_dict, continuous_dict,\n",
    "                                               add_frame_features=False,\n",
    "                                               subphone_features=None).astype(np.float32)\n",
    "\n",
    "    # Apply normalization\n",
    "    ty = \"duration\"\n",
    "    duration_linguistic_features = minmax_scale(duration_linguistic_features, \n",
    "                                       X_min[ty], X_max[ty], feature_range=(0.01, 0.99))\n",
    "    \n",
    "    # Apply models\n",
    "    duration_model = duration_model.cpu()\n",
    "    duration_model.eval()\n",
    "    \n",
    "    #  Apply model\n",
    "    x = Variable(torch.from_numpy(duration_linguistic_features)).float()\n",
    "    try:\n",
    "        duration_predicted = duration_model(x).data.numpy()\n",
    "    except:\n",
    "        h, c = duration_model.init_hidden(batch_size=1)\n",
    "        xl = len(x)\n",
    "        x = x.view(1, -1, x.size(-1))\n",
    "        duration_predicted = duration_model(x, [xl], h, c).data.numpy()\n",
    "        duration_predicted = duration_predicted.reshape(-1, duration_predicted.shape[-1])\n",
    "                     \n",
    "    # Apply denormalization\n",
    "    duration_predicted = duration_predicted * Y_scale[ty] + Y_mean[ty]\n",
    "    duration_predicted = np.round(duration_predicted)\n",
    "    \n",
    "    # Set minimum state duration to 1\n",
    "    duration_predicted[duration_predicted <= 0] = 1\n",
    "    hts_labels.set_durations(duration_predicted)\n",
    "    \n",
    "    return hts_labels    \n",
    "\n",
    "\n",
    "def test_one_utt(hts_labels, duration_model, acoustic_model, post_filter=True):\n",
    "    # Predict durations\n",
    "    duration_modified_hts_labels = gen_duration(hts_labels, duration_model)\n",
    "    \n",
    "    # Linguistic features\n",
    "    linguistic_features = fe.linguistic_features(duration_modified_hts_labels, \n",
    "                                                  binary_dict, continuous_dict,\n",
    "                                                  add_frame_features=True,\n",
    "                                                  subphone_features=acoustic_subphone_features)\n",
    "    # Trim silences\n",
    "    indices = duration_modified_hts_labels.silence_frame_indices()\n",
    "    linguistic_features = np.delete(linguistic_features, indices, axis=0)\n",
    "\n",
    "    # Apply normalization\n",
    "    ty = \"acoustic\"\n",
    "    linguistic_features = minmax_scale(linguistic_features, \n",
    "                                       X_min[ty], X_max[ty], feature_range=(0.01, 0.99))\n",
    "    \n",
    "    # Predict acoustic features\n",
    "    acoustic_model = acoustic_model.cpu()\n",
    "    acoustic_model.eval()\n",
    "    x = Variable(torch.from_numpy(linguistic_features)).float()\n",
    "    try:\n",
    "        acoustic_predicted = acoustic_model(x).data.numpy()\n",
    "    except:\n",
    "        h, c = acoustic_model.init_hidden(batch_size=1)\n",
    "        xl = len(x)\n",
    "        x = x.view(1, -1, x.size(-1))\n",
    "        acoustic_predicted = acoustic_model(x, [xl], h, c).data.numpy()\n",
    "        acoustic_predicted = acoustic_predicted.reshape(-1, acoustic_predicted.shape[-1])\n",
    "             \n",
    "    # Apply denormalization\n",
    "    acoustic_predicted = acoustic_predicted * Y_scale[ty] + Y_mean[ty]\n",
    "    \n",
    "    return gen_waveform(acoustic_predicted, post_filter)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_parameters(y_predicted):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "\n",
    "    #mgc = np.zeros(mgc.shape)\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.ones(lf0.shape) * 4\n",
    "    vuv = y_predicted[:,vuv_start_idx]#声帯を使わない囁き声になる\n",
    "    #vuv = np.zeros(vuv.shape[0])\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    #bap = np.zeros(bap.shape)\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(Y_var[ty][:lf0_start_idx], (T, 1))\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(Y_var[ty][lf0_start_idx:vuv_start_idx], (T,1))\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(Y_var[ty][bap_start_idx:], (T, 1))\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(Audio(gen_waveform(Y['acoustic']['train'][0], True), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen generated audio\n",
    "\n",
    "Generated audio samples with Merlin's slt_full_demo are attached. You can compare them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MyRNN\")\n",
    "label_path = './data/basic5000/test_phone_align/BASIC5000_4501.lab'\n",
    "hts_labels = hts.load(label_path)\n",
    "waveform = test_one_utt(hts_labels, models[\"duration\"], models[\"acoustic\"])\n",
    "#wavfile.write(join(save_dir, basename(wav_path1)), rate=fs, data=waveform)\n",
    "IPython.display.display(Audio(waveform, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_paths = sorted(glob(join(DATA_ROOT, \"test_phone_align\", \"*.lab\")))\n",
    "ffn_generated_wav_files = sorted(glob(join(\"./generated/jp-01-tts/*.wav\")))\n",
    "hts_generated_wav_files = sorted(glob(join(\"./generated/hts_nit_atr503_2mix/*.wav\")))\n",
    "\n",
    "# Save generated wav files for later comparizon\n",
    "save_dir = join(\"./generated/jp-02-tts\")\n",
    "if not exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "for label_path  in test_label_paths:\n",
    "\n",
    "    print(\"MyRNN\")\n",
    "    hts_labels = hts.load(label_path)\n",
    "    waveform = test_one_utt(hts_labels, models[\"duration\"], models[\"acoustic\"])\n",
    "    #wavfile.write(join(save_dir, basename(wav_path1)), rate=fs, data=waveform)\n",
    "    IPython.display.display(Audio(waveform, rate=fs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS using OpenJTalk frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the OpenJTalk text processing frontend, we can generate speech for any input text. In this notebook, we use https://github.com/r9y9/pyopenjtalk to use OpenJtalk frontend functionality.\n",
    "\n",
    "注意: 現時点では、nnmnkwiiとpyopenjtalkの開発版が必要です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../pyopenjtalk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('Users/kazuya_yufune/Desktop/猿渡研究室/チュートリアル/nnmnkwii_gallery/notebooks/pyopenjtalk/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopenjtalk\n",
    "\n",
    "for idx, text in enumerate([\n",
    "    \"こんにちは。本日は、お越しいただき誠にありがとうございます。\",\n",
    "    \"拙者は、サムライでございまする。\",\n",
    "    \"最後まで読んでいただき、本当にありがとうございます。\",\n",
    "    \"フィードバックがあれば、ぜひお教えくださいませ。\",\n",
    "    \"音声合成の実現には、自然言語処理、機械学習、音声信号処理など、複数の分野に渡る知識が必要です\",\n",
    "    \"このチュートリアルですべてをカバーしているわけではありませんが、少しでも学習の助けになれば幸いです。\",\n",
    "    \"ありがとうございました！\",\n",
    "]):\n",
    "    _, labels = pyopenjtalk.run_frontend(text)\n",
    "    hts_labels = hts.load(lines=labels)\n",
    "    print(idx, text)\n",
    "    waveform = test_one_utt(hts_labels, models[\"duration\"], models[\"acoustic\"])\n",
    "    IPython.display.display(Audio(waveform, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunbo = np.sqrt(0.1) * ( np.pi / 2 -   np.arctan(1/np.sqrt(0.1))) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/bunbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "root_gamma = np.sqrt(gamma)\n",
    "p1 = 100\n",
    "p2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunbo1 = root_gamma*(np.pi/2 - np.arctan(1/root_gamma) ) + np.sqrt(p2*gamma / p1) * (np.pi/2 - np.arctan(np.sqrt(p1/p2/gamma))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/ bunbo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunbo2 = root_gamma*(np.pi/2 - np.arctan(1/root_gamma) ) + np.sqrt(p1*gamma / p2) * (np.pi/2 - np.arctan(np.sqrt(p2/p1/gamma))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / bunbo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunbo1_4 = root_gamma*(np.pi/2 - np.arctan(1/root_gamma) ) + np.sqrt(p2*gamma / p1) * (np.pi/2 - np.arctan(np.sqrt(p2/p1/gamma))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunbo2_4 = root_gamma*(np.pi/2 - np.arctan(1/root_gamma) ) + np.sqrt(p1*gamma / p2) * (np.pi/2 - np.arctan(np.sqrt(p1/p2/gamma))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/bunbo1_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / bunbo2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
